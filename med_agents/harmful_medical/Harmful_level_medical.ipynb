{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-Tune a SLM for medical domain, which will predict if the user utterances are benign or harmless. Supervised fine-tunung was performed using PEFT-LORA."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data For fune-tuning: [CARES](https://arxiv.org/pdf/2505.11413)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"HFXM/CARES-18K\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "display(ds)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def categorize_harmful_level(level):\n",
    "    if level in [0, 1]:\n",
    "        return \"harmless\"\n",
    "    elif level in [2, 3]:\n",
    "        return \"harmful\"\n",
    "    else:\n",
    "        return \"unknown\"\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "harmful_levels = pd.Series(ds['train']['harmful_level'])\n",
    "display(harmful_levels.value_counts())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load gemma smallest LLM and use prompt to predict the harmful label"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Load the Gemma 2B instruction-tuned model and its corresponding tokenizer from the `transformers` library, and then use these components to generate harmful label predictions based on the `base_prompt` and `prompt` fields from the `CARES-18K` dataset. Finally, evaluate the model's predictions by comparing them against the `harmful_binary` labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gemma Model and Tokenizer\n",
    "\n",
    "### Subtask:\n",
    "Load the smallest Gemma Large Language Model and its corresponding tokenizer from the Hugging Face `transformers` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = 'google/gemma-2b-it'\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto'\n",
    ")\n",
    "print(\"Model loaded successfully.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for LLM Inference\n",
    "\n",
    "### Subtask:\n",
    "Formulate prompts for the LLM using the `base_prompt` and `prompt` fields from the dataset, instructing the model to predict the harmful level of the given text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "The previous code blocks repeatedly caused `SyntaxError` because markdown instructions were placed in Python cells. This time, I will provide a Python code block that correctly extracts the necessary data, formats it into prompts suitable for the Gemma model using the tokenizer's chat template, and then displays the first few prepared prompts as instructed, fulfilling the current subtask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_data = ds['train']\n",
    "test = ds['test']\n",
    "\n",
    "# apply categorization to train and test datasets\n",
    "train_harmful_levels = [categorize_harmful_level(level) for level in train_data['harmful_level']]\n",
    "test_harmful_levels = [categorize_harmful_level(level) for level in test['harmful_level']]\n",
    "\n",
    "# Add new column using add_column method\n",
    "train_data = train_data.add_column(\"harm_binary\", train_harmful_levels)\n",
    "test = test.add_column(\"harm_binary\", test_harmful_levels)\n",
    "\n",
    "# sub-sample train_data for quicker processing during testing\n",
    "subset_size = 1000\n",
    "train_data = train_data.select(range(subset_size))\n",
    "test = test.select(range(subset_size))\n",
    "\n",
    "display(pd.Series(train_data[\"harm_binary\"]).value_counts())\n",
    "display(pd.Series(test[\"harm_binary\"]).value_counts())"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Harmful Label Prediction\n",
    "\n",
    "### Subtask:\n",
    "Use the loaded Gemma model and tokenizer to perform inference on the prepared prompts and generate predictions for the 'harmful_label'.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "harmful_predictions= []\n",
    "\n",
    "for i in range(30):\n",
    "    base_p = test[i]['base_prompt']\n",
    "    p = test[i]['prompt']\n",
    "\n",
    "    # Construct the user message part\n",
    "    messages = [\n",
    "        {\"role\": \"user\"\n",
    "        , \"content\": f\"{base_p}\\n\\nBased on the above context, classify the harm label of the following text as harmful or harmless DIRECTLY:\\n\\nText: \\\"{p}\\\"\\n\\n<label>harmless/harmful</label>:\"}\n",
    "    ]\n",
    "\n",
    "    user_prompt_formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    inference_prompt = user_prompt_formatted + '<start_of_turn>assistant\\n'\n",
    "\n",
    "    # Tokenize the prepared inference prompt\n",
    "    input_ids = tokenizer(inference_prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate text responses, allowing for a longer response (max_new_tokens=10)\n",
    "    # force_words_ids and num_beams are removed as they were not effective for single-token output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=10, # Allow more tokens to observe full response\n",
    "            do_sample=False, # For deterministic output\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "        )\n",
    "\n",
    "    generated_token_ids = outputs[0, input_ids['input_ids'].shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Update regex to search for 'harmless' or 'harmful'\n",
    "    match = re.search(r'\\b(harmless|harmful)\\b', generated_text)\n",
    "    if match:\n",
    "        predicted_category = match.group(1)\n",
    "        if predicted_category == 'harmless':\n",
    "            predicted_level = 0\n",
    "        elif predicted_category == 'harmful':\n",
    "            predicted_level = 1\n",
    "        else:\n",
    "            predicted_level = -1 # Should not happen if regex is correct\n",
    "    else:\n",
    "        predicted_level = -1 # Indicate no clear prediction\n",
    "\n",
    "    harmful_predictions.append(predicted_level)\n",
    "\n",
    "    # print(f\"\\n--- Prediction for Prompt {i+1} ---\")\n",
    "    # print(f\"Inference Prompt (last 100 chars): '{inference_prompt[-100:]}'\")\n",
    "    # print(f\"Raw Generated Text: '{generated_text.replace('\\n', '\\\\n')}'\")\n",
    "    # print(f\"Predicted Harmful Level: {predicted_level}\")\n",
    "\n",
    "print(f\"\\nGenerated Harmful Predictions for first {subset_size} samples: {harmful_predictions}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# calculate accuracy on the subset\n",
    "true_labels_str = train_data['harm_binary'][:subset_size]\n",
    "\n",
    "# Convert true_labels to numerical format (0 for harmless, 1 for harmful, -1 for unknown)\n",
    "true_labels_numeric = [0 if label == \"harmless\" else (1 if label == \"harmful\" else -1) for label in true_labels_str]\n",
    "\n",
    "# Filter out samples where either true label or prediction is -1 (unknown/unclear)\n",
    "filtered_true_labels = []\n",
    "filtered_predictions = []\n",
    "for true, pred in zip(true_labels_numeric, harmful_predictions):\n",
    "    if true in [0, 1] and pred in [0, 1]: # Only include valid true labels and predictions\n",
    "        filtered_true_labels.append(true)\n",
    "        filtered_predictions.append(pred)\n",
    "\n",
    "# Calculate accuracy on the filtered subset\n",
    "if len(filtered_predictions) > 0:\n",
    "    correct_predictions = sum(1 for true, pred in zip(filtered_true_labels, filtered_predictions) if true == pred)\n",
    "    accuracy = correct_predictions / len(filtered_predictions)\n",
    "    print(f\"\\nAccuracy on the filtered subset of {len(filtered_predictions)} samples: {accuracy:.2%}\")\n",
    "else:\n",
    "    print(\"\\nNo valid samples (with known true labels and valid predictions) to calculate accuracy.\")\n",
    "\n",
    "# f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "if len(filtered_predictions) > 0:\n",
    "    f1 = f1_score(filtered_true_labels, filtered_predictions, average='weighted')\n",
    "    print(f\"F1 Score on the filtered subset of {len(filtered_predictions)} samples: {f1:.2%}\")\n",
    "else:\n",
    "    print(\"No valid samples (with known true labels and valid predictions) to calculate F1 Score.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def map_response( row,  system_message=None):\n",
    "    base_p, p, hallu_label, = row['base_prompt'], row['prompt'], row['harm_binary']\n",
    "    # Format chat using tokenizer's chat template\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "\n",
    "    # # We assume the data are all single-turn conversation\n",
    "    # messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "\n",
    "    tmp = {\"prompt\":[{\"role\": \"user\"\n",
    "    , \"content\": f\"{base_p}\\n\\nBased on the above context, classify the harmful label of the following text as harmful or harmless DIRECTLY:\\n\\nText: \\\"{p}\\\"\\n\\n<label>harmless/harmful</label>:\"}\n",
    "                    ],\n",
    "          \"completion\": [{\"role\": \"assistant\"\n",
    "                          , \"content\": f\"{hallu_label}\"}\n",
    "                ]\n",
    "           }\n",
    "\n",
    "    return tmp\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_map = train_data.map(map_response , remove_columns=[\"generation_model\", \"principle_index\", \"harmful_level\", \"method\", \"base_prompt\", \"harm_binary\"])\n",
    "print(next(iter(train_map)))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q trl bitsandbytes peft"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer, SFTConfig"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank of the update matrices. Lower rank results in smaller models.\n",
    "    lora_alpha=32, # Alpha parameter for LoRA scaling. A higher alpha value assigns more weight to the LoRA layers.\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Target specific modules to apply LoRA. These are common layers in transformer models.\n",
    "    lora_dropout=0.05, # Dropout probability for LoRA layers.\n",
    "    bias=\"none\", # Type of bias to be applied. \"none\" means no bias. Other options include \"all\" or \"lora_only\".\n",
    "    task_type=\"CAUSAL_LM\", # Task type, indicating that the model is for causal language modeling.\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"Model converted to PEFT model with LoRA configuration.\")\n",
    "model.print_trainable_parameters()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "With the PEFT (LoRA) model now configured, I need to define the `SFTConfig` once again, ensuring `gradient_checkpointing` is set to `True` for continued memory optimization and `report_to=['wandb']` is included. This configuration will then be used to initialize the `SFTTrainer` with the newly adapted PEFT model and the `train_map` dataset, and finally, start the training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sft_config = SFTConfig(\n",
    "    learning_rate=3e-5, # Learning rate for training.\n",
    "    num_train_epochs=1, #  Set the number of epochs to train the model.\n",
    "    per_device_train_batch_size=1, # Batch size for each device (e.g., GPU) during training.\n",
    "    gradient_accumulation_steps=4, # Number of steps before performing a backward/update pass to accumulate gradients.\n",
    "    gradient_checkpointing=True, # Enable gradient checkpointing to reduce memory usage during training at the cost of slower training speed.\n",
    "    logging_steps=2,  # Frequency of logging training progress (log every 2 steps).\n",
    "    report_to=[\"wandb\"], # Integrate Weights & Biases logging.\n",
    "    run_name=\"my-lora-sft-run\",\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_map,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "sft_trainer.train()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "The model has now been successfully fine-tuned using LoRA and gradient checkpointing, as evidenced by the `TrainOutput` from the previous step. To evaluate the impact of this fine-tuning, I need to re-generate harmful label predictions using the trained model on the `prepared_prompts` and then re-calculate the accuracy and F1 score. This step directly addresses the final part of the main task, which is to evaluate the model's predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "subset_size =50\n",
    "harmful_predictions= []\n",
    "for i in range(subset_size):\n",
    "    base_p = test[i]['base_prompt']\n",
    "    p = test[i]['prompt']\n",
    "\n",
    "  # Construct the user message part\n",
    "    messages = [\n",
    "        {\"role\": \"user\"\n",
    "        , \"content\": f\"{base_p}\\n\\nBased on the above context, classify the harm label of the following text as harmful or harmless DIRECTLY:\\n\\nText: \\\"{p}\\\"\\n\\n<label>harmless/harmful</label>:\"}\n",
    "    ]\n",
    "\n",
    "    user_prompt_formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    inference_prompt = user_prompt_formatted + '<start_of_turn>assistant\\n'\n",
    "\n",
    "    # Tokenize the prepared inference prompt\n",
    "    input_ids = tokenizer(inference_prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate text responses, allowing for a longer response (max_new_tokens=10)\n",
    "    # force_words_ids and num_beams are removed as they were not effective for single-token output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=10, # Allow more tokens to observe full response\n",
    "            do_sample=False, # For deterministic output\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "        )\n",
    "\n",
    "    generated_token_ids = outputs[0, input_ids['input_ids'].shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Update regex to search for 'harmless' or 'harmful'\n",
    "    match = re.search(r'\\b(harmless|harmful)\\b', generated_text)\n",
    "    if match:\n",
    "        predicted_category = match.group(1)\n",
    "        if predicted_category == 'harmless':\n",
    "            predicted_level = 0\n",
    "        elif predicted_category == 'harmful':\n",
    "            predicted_level = 1\n",
    "        else:\n",
    "            predicted_level = -1 # Should not happen if regex is correct\n",
    "    else:\n",
    "        predicted_level = -1 # Indicate no clear prediction\n",
    "\n",
    "    harmful_predictions.append(predicted_level)\n",
    "\n",
    "    print(f\"\\n--- Prediction for Prompt {i+1} ---\")\n",
    "    print(f\"Inference Prompt (last 100 chars): '{inference_prompt[-100:]}'\")\n",
    "    print(f\"Raw Generated Text: '{generated_text.replace('\\n', '\\\\n')}'\")\n",
    "    print(f\"Predicted Harmful Level: {predicted_level}\")\n",
    "\n",
    "print(f\"\\nGenerated Harmful Predictions for first {subset_size} samples: {harmful_predictions}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Fine-tuned Model\n",
    "\n",
    "### Subtask:\n",
    "After the SFT training is complete, use the fine-tuned model to make predictions on the test set and evaluate its performance using appropriate metrics like accuracy and F1-score, comparing them to the `harm_binary` labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# calculate accuracy on the subset\n",
    "true_labels_str = test['harm_binary'][:subset_size]\n",
    "\n",
    "# Convert true_labels to numerical format (0 for harmless, 1 for harmful, -1 for unknown)\n",
    "true_labels_numeric = [0 if label == \"harmless\" else (1 if label == \"harmful\" else -1) for label in true_labels_str]\n",
    "\n",
    "# Filter out samples where either true label or prediction is -1 (unknown/unclear)\n",
    "filtered_true_labels = []\n",
    "filtered_predictions = []\n",
    "for true, pred in zip(true_labels_numeric, harmful_predictions):\n",
    "    if true in [0, 1] and pred in [0, 1]: # Only include valid true labels and predictions\n",
    "        filtered_true_labels.append(true)\n",
    "        filtered_predictions.append(pred)\n",
    "\n",
    "# Calculate accuracy on the filtered subset\n",
    "if len(filtered_predictions) > 0:\n",
    "    correct_predictions = sum(1 for true, pred in zip(filtered_true_labels, filtered_predictions) if true == pred)\n",
    "    accuracy = correct_predictions / len(filtered_predictions)\n",
    "    print(f\"\\nAccuracy on the filtered subset of {len(filtered_predictions)} samples: {accuracy:.2%}\")\n",
    "else:\n",
    "    print(\"\\nNo valid samples (with known true labels and valid predictions) to calculate accuracy.\")\n",
    "\n",
    "# f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "if len(filtered_predictions) > 0:\n",
    "    f1 = f1_score(filtered_true_labels, filtered_predictions, average='weighted')\n",
    "    print(f\"F1 Score on the filtered subset of {len(filtered_predictions)} samples: {f1:.2%}\")\n",
    "else:\n",
    "    print(\"No valid samples (with known true labels and valid predictions) to calculate F1 Score.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Future: test with Out-of-Domain data of Patient Safety:https://arxiv.org/abs/2507.07248\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds_ood = load_dataset(\"microsoft/PatientSafetyBench\")\n",
    "display(ds_ood)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Repetitive Model Responses\n",
    "\n",
    "### Subtask:\n",
    "Examine the tokenizer's behavior for 'harmless' and 'harmful' to understand if they are split into sub-word tokens. This could explain the repetitive outputs like 'harharhar...'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "To understand the repetitive model responses, I will examine how the tokenizer processes the words 'harmless' and 'harmful' by encoding them into token IDs and then decoding those IDs back into their sub-word components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Tokenizing 'harmless':\")\n",
    "harmless_tokens = tokenizer.encode('harmless', add_special_tokens=False)\n",
    "print(f\"Token IDs for 'harmless': {harmless_tokens}\")\n",
    "print(f\"Decoded tokens for 'harmless': {[tokenizer.decode([t]) for t in harmless_tokens]}\\n\")\n",
    "\n",
    "print(\"Tokenizing 'harmful':\")\n",
    "harmful_tokens = tokenizer.encode('harmful', add_special_tokens=False)\n",
    "print(f\"Token IDs for 'harmful': {harmful_tokens}\")\n",
    "print(f\"Decoded tokens for 'harmful': {[tokenizer.decode([t]) for t in harmful_tokens]}\\n\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization analysis reveals that words like 'harmless' and 'harmful' are not treated as single tokens but are broken down into sub-word units by the Gemma tokenizer.\n",
    "\n",
    "- **'harmless'** is tokenized into `['har', 'mless']`.\n",
    "- **'harmful'** is tokenized into `['harm', 'ful']`.\n",
    "\n",
    "This sub-word tokenization explains the repetitive output observed during inference (e.g., 'harharharhar' or 'harmharmharm'). When the model attempts to generate 'harmless' or 'harmful', it generates the first sub-word ('har' or 'harm'). If the model then struggles to predict the correct subsequent sub-word ('mless' or 'ful') due to insufficient fine-tuning or a lack of strong signal in the training data for these specific completions, it might default to repeating the initial, already generated sub-word. This results in outputs like 'harharhar...' or 'harmharmharm...' because the model gets stuck in a loop of generating the most probable single sub-word given the preceding context, rather than completing the full word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization analysis reveals that words like 'harmless' and 'harmful' are not treated as single tokens but are broken down into sub-word units by the Gemma tokenizer.\n",
    "\n",
    "- **'harmless'** is tokenized into `['har', 'mless']`.\n",
    "- **'harmful'** is tokenized into `['harm', 'ful']`.\n",
    "\n",
    "This sub-word tokenization explains the repetitive output observed during inference (e.g., 'harharharhar' or 'harmharmharm'). When the model attempts to generate 'harmless' or 'harmful', it generates the first sub-word ('har' or 'harm'). If the model then struggles to predict the correct subsequent sub-word ('mless' or 'ful') due to insufficient fine-tuning or a lack of strong signal in the training data for these specific completions, it might default to repeating the initial, already generated sub-word. This results in outputs like 'harharhar...' or 'harmharmharm...' because the model gets stuck in a loop of generating the most probable single sub-word given the preceding context, rather than completing the full word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine SFT Training Target to Numerical Output\n",
    "\n",
    "### Subtask:\n",
    "Modify the `map_response` function to train the model to output a single digit '0' (for harmless) or '1' (for harmful) directly, rather than the full words. This is often more robust for classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "The current `map_response` function generates 'harmful' or 'harmless' string labels. To align with the goal of numerical output for a more robust classification, I need to redefine `map_response` to convert these string labels into '0' for 'harmless' and '1' for 'harmful' in the `completion` field.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def map_response(row, system_message=None):\n",
    "    base_p, p, hallu_label = row['base_prompt'], row['prompt'], row['harm_binary']\n",
    "\n",
    "    # Convert string labels to numerical labels\n",
    "    numerical_label = '0' if hallu_label == 'harmless' else '1'\n",
    "\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "\n",
    "    tmp = {\n",
    "        \"prompt\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{base_p}\\n\\nBased on the above context, classify the harmful label of the following text as harmful or harmless DIRECTLY:\\n\\nText: \\\"{p}\\\"\\n\\n<label>harmless/harmful</label>:\"\n",
    "        }],\n",
    "        \"completion\": [{\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"{numerical_label}\"\n",
    "        }]\n",
    "    }\n",
    "    return tmp\n",
    "\n",
    "print(\"Updated `map_response` function defined.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "With the `map_response` function now updated to produce numerical labels, I need to re-apply this function to the `train_data` to generate a new `train_map` dataset. This step is crucial for preparing the data with the new numerical targets for subsequent fine-tuning with the SFTTrainer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_map = train_data.map(map_response , remove_columns=[\"generation_model\", \"principle_index\", \"harmful_level\", \"method\", \"base_prompt\", \"harm_binary\"])\n",
    "print(next(iter(train_map)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "With the `train_map` dataset now prepared with numerical completion targets, I need to re-initialize the `SFTTrainer` with this updated dataset and restart the fine-tuning process. This ensures the model is trained with the new numerical output format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sft_config = SFTConfig(\n",
    "    learning_rate=3e-5, # Learning rate for training.\n",
    "    num_train_epochs=1, #  Set the number of epochs to train the model.\n",
    "    per_device_train_batch_size=1, # Batch size for each device (e.g., GPU) during training.\n",
    "    gradient_accumulation_steps=4, # Number of steps before performing a backward/update pass to accumulate gradients.\n",
    "    gradient_checkpointing=True, # Enable gradient checkpointing to reduce memory usage during training at the cost of slower training speed.\n",
    "    logging_steps=2,  # Frequency of logging training progress (log every 2 steps).\n",
    "    report_to=[\"wandb\"], # Integrate Weights & Biases logging.\n",
    "    run_name=\"my-lora-sft-run-numerical\",\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_map,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "sft_trainer.train()\n",
    "print(\"SFTTrainer re-initialized and training started with numerical targets.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Adjust the `learning_rate` in `sft_config` to a smaller value (e.g., `1e-5` or `5e-6`), re-initialize the `SFTTrainer` with the modified configuration and the `train_map` dataset, and then execute `sft_trainer.train()` to fine-tune the model with the adjusted learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Training Hyperparameters\n",
    "\n",
    "### Subtask:\n",
    "Modify the `sft_config` to reduce the `learning_rate` to a smaller value (e.g., from `3e-5` to `1e-5` or `5e-6`) to prevent divergence and help the model converge more effectively during fine-tuning. Keep `gradient_checkpointing=True` for memory optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "To adjust the training hyperparameters as requested, I need to define a new `SFTConfig` with a reduced learning rate and an updated `run_name`, then re-initialize the `SFTTrainer` with this new configuration and the previously prepared `train_map` dataset, and finally, start the training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sft_config = SFTConfig(\n",
    "    learning_rate=1e-5, # Reduced learning rate for training.\n",
    "    num_train_epochs=1, #  Set the number of epochs to train the model.\n",
    "    per_device_train_batch_size=1, # Batch size for each device (e.g., GPU) during training.\n",
    "    gradient_accumulation_steps=4, # Number of steps before performing a backward/update pass to accumulate gradients.\n",
    "    gradient_checkpointing=True, # Enable gradient checkpointing to reduce memory usage during training at the cost of slower training speed.\n",
    "    logging_steps=2,  # Frequency of logging training progress (log every 2 steps).\n",
    "    report_to=[\"wandb\"], # Integrate Weights & Biases logging.\n",
    "    run_name=\"my-lora-sft-run-numerical-lr-1e-5\", # Updated run name\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_map,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "sft_trainer.train()\n",
    "print(\"SFTTrainer re-initialized and training started with numerical targets and reduced learning rate.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Adjust the inference prompt and generation parameters to make the model output a single numerical digit ('0' or '1'), then re-run predictions on the test set and calculate accuracy and F1 score to evaluate the fine-tuned model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Inference Prompt and Generation Parameters (Revisit)\n",
    "\n",
    "### Subtask:\n",
    "Re-evaluate and potentially re-adjust the inference prompt formatting and generation parameters (`max_new_tokens`, `force_words_ids`, `num_beams`) to ensure the model generates only a single numerical digit ('0' or '1') during prediction, addressing the previous repetitive or empty output issues observed during inference on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "To evaluate the fine-tuned model's ability to predict numerical harmful labels, I need to perform inference on the test dataset using the updated prompt format and generation parameters to specifically target single-digit outputs ('0' or '1').\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Using the test subset_size from previous cells, which was 50.\n",
    "# The 'test' dataset was defined in cell 'm3eAfzyRz5yt'\n",
    "# and subset_size was set for the 'test' dataset in cell 'affc3367'.\n",
    "\n",
    "harmful_predictions_numerical = []\n",
    "\n",
    "for i in range(subset_size):\n",
    "    base_p = test[i]['base_prompt']\n",
    "    p = test[i]['prompt']\n",
    "\n",
    "    # Construct the user message part with numerical label instruction\n",
    "    messages = [\n",
    "        {\"role\": \"user\"\n",
    "        , \"content\": f\"{base_p}\\n\\nBased on the above context, classify the harmful label of the following text as 0 for harmless or 1 for harmful DIRECTLY:\\n\\nText: \\\"{p}\\\"\\n\\n<label>0/1</label>:\"}\n",
    "    ]\n",
    "\n",
    "    user_prompt_formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    inference_prompt = user_prompt_formatted + '<start_of_turn>assistant\\n'\n",
    "\n",
    "    # Tokenize the prepared inference prompt\n",
    "    input_ids = tokenizer(inference_prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate text responses, limiting to a single new token\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=1, # Limit to a single token output\n",
    "            do_sample=False, # For deterministic output\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_token_ids = outputs[0, input_ids['input_ids'].shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Use regex to search for '0' or '1'\n",
    "    match = re.search(r'\\b(0|1)\\b', generated_text)\n",
    "    if match:\n",
    "        predicted_level = int(match.group(1))\n",
    "    else:\n",
    "        predicted_level = -1 # Indicate no clear numerical prediction\n",
    "\n",
    "    harmful_predictions_numerical.append(predicted_level)\n",
    "\n",
    "    print(f\"\\n--- Prediction for Prompt {i+1} ---\")\n",
    "    print(f\"Inference Prompt (last 100 chars): '{inference_prompt[-100:]}'\")\n",
    "    print(f\"Raw Generated Text: '{generated_text.replace('\\n', '\\n')}'\")\n",
    "    print(f\"Predicted Harmful Level (numerical): {predicted_level}\")\n",
    "\n",
    "print(f\"\\nGenerated Harmful Predictions (numerical) for first {subset_size} samples: {harmful_predictions_numerical}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "With the numerical predictions generated, I need to calculate the accuracy and F1 score by comparing these predictions against the true `harm_binary` labels from the test dataset. This will evaluate the fine-tuned model's performance with the refined numerical output target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Convert true_labels to numerical format (0 for harmless, 1 for harmful)\n",
    "# Using the 'test' dataset, which was defined in cell 'm3eAfzyRz5yt'\n",
    "# and subset_size was set for the 'test' dataset in cell 'affc3367'.\n",
    "true_labels_str_test = test['harm_binary'][:subset_size]\n",
    "true_labels_numerical_test = [0 if label == \"harmless\" else 1 for label in true_labels_str_test]\n",
    "\n",
    "# Filter out samples where prediction is -1 (unknown/unclear)\n",
    "filtered_true_labels_numerical = []\n",
    "filtered_predictions_numerical = []\n",
    "for true, pred in zip(true_labels_numerical_test, harmful_predictions_numerical):\n",
    "    if pred in [0, 1]: # Only include valid predictions\n",
    "        filtered_true_labels_numerical.append(true)\n",
    "        filtered_predictions_numerical.append(pred)\n",
    "\n",
    "# Calculate accuracy on the filtered subset\n",
    "if len(filtered_predictions_numerical) > 0:\n",
    "    correct_predictions = sum(1 for true, pred in zip(filtered_true_labels_numerical, filtered_predictions_numerical) if true == pred)\n",
    "    accuracy = correct_predictions / len(filtered_predictions_numerical)\n",
    "    print(f\"\\nAccuracy on the filtered test subset of {len(filtered_predictions_numerical)} samples: {accuracy:.2%}\")\n",
    "else:\n",
    "    print(\"\\nNo valid samples (with known true labels and valid numerical predictions) to calculate accuracy.\")\n",
    "\n",
    "# Calculate F1 score\n",
    "if len(filtered_predictions_numerical) > 0:\n",
    "    f1 = f1_score(filtered_true_labels_numerical, filtered_predictions_numerical, average='weighted')\n",
    "    print(f\"F1 Score on the filtered test subset of {len(filtered_predictions_numerical)} samples: {f1:.2%}\")\n",
    "else:\n",
    "    print(\"No valid samples (with known true labels and valid numerical predictions) to calculate F1 Score.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Findings and Next Steps\n",
    "\n",
    "### Subtask:\n",
    "Provide a comprehensive summary of the fine-tuning process, the model's performance on both training and test sets, and key observations, including any discrepancies and recommendations for further improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "### Q&A\n",
    "The adjustment of the inference prompt and generation parameters successfully enabled the model to output a single numerical digit ('0' or '1') as required. However, the model's classification performance on the test set was low, with an accuracy of 48.00% and an F1 score of 35.12%.\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "*   The model, after adjusting the prompt and setting `max_new_tokens=1`, consistently generated a single numerical digit ('0' or '1') for all 50 samples in the test subset, successfully addressing previous issues with repetitive or empty outputs.\n",
    "*   Out of 50 test samples, the model predicted 48 as '0' (harmless) and 2 as '1' (harmful).\n",
    "*   The accuracy on the filtered test subset of 50 samples was calculated to be 48.00%.\n",
    "*   The F1 Score on the filtered test subset of 50 samples was calculated to be 35.12%.\n",
    "\n",
    "### Insights or Next Steps\n",
    "*   While the model now produces the desired output format, its low accuracy and F1 score suggest that its classification capabilities for harmful content are still subpar. Further fine-tuning or a more robust training dataset might be necessary.\n",
    "*   Investigate the cases where the model made incorrect predictions to understand specific error patterns. This could involve analyzing the prompts and corresponding true labels for misclassified samples.\n"
   ]
  }
 ]
}